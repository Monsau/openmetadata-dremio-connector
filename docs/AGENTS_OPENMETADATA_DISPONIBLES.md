# ü§ñ Agents OpenMetadata Disponibles

## Vue d'ensemble

OpenMetadata propose plusieurs types d'agents/workflows pour collecter des m√©tadonn√©es et enrichir votre catalogue de donn√©es. Voici la liste compl√®te des agents disponibles **autres que DBT**.

---

## üìä 1. Metadata Ingestion Agents

Ces agents collectent les m√©tadonn√©es (tables, colonnes, sch√©mas) depuis diff√©rentes sources.

### 1.1 Databases (JDBC)

#### ‚úÖ Dremio (D√âJ√Ä IMPL√âMENT√â)
- **√âtat** : ‚úÖ Actif et fonctionnel
- **Type** : Custom connector via REST API
- **Capacit√©s** : Metadata + Profiling + Classification
- **Documentation** : `AGENT_METADATA_COMPLET.md`

#### PostgreSQL
- **Type** : JDBC connector
- **Capacit√©s** : 
  - Metadata ingestion
  - Profiling
  - Query usage
  - Lineage
- **Configuration** :
```yaml
source:
  type: Postgres
  serviceName: postgres-prod
  serviceConnection:
    config:
      type: Postgres
      username: <username>
      password: <password>
      hostPort: localhost:5432
      database: postgres
```

#### MySQL / MariaDB
- **Type** : JDBC connector
- **Capacit√©s** : Metadata, Profiling, Usage, Lineage
- **Configuration** :
```yaml
source:
  type: Mysql
  serviceName: mysql-prod
  serviceConnection:
    config:
      type: Mysql
      username: <username>
      password: <password>
      hostPort: localhost:3306
      databaseSchema: <database>
```

#### Oracle
- **Type** : JDBC connector
- **Capacit√©s** : Metadata, Profiling, Usage
- **Configuration** :
```yaml
source:
  type: Oracle
  serviceName: oracle-prod
  serviceConnection:
    config:
      type: Oracle
      username: <username>
      password: <password>
      hostPort: localhost:1521
      oracleConnectionType:
        oracleServiceName: <service_name>
```

#### Microsoft SQL Server
- **Type** : JDBC connector
- **Capacit√©s** : Metadata, Profiling, Usage, Lineage
- **Configuration** :
```yaml
source:
  type: Mssql
  serviceName: mssql-prod
  serviceConnection:
    config:
      type: Mssql
      username: <username>
      password: <password>
      hostPort: localhost:1433
      database: <database>
```

#### Snowflake
- **Type** : Cloud Data Warehouse
- **Capacit√©s** : Metadata, Profiling, Usage, Lineage, Query logs
- **Configuration** :
```yaml
source:
  type: Snowflake
  serviceName: snowflake-prod
  serviceConnection:
    config:
      type: Snowflake
      username: <username>
      password: <password>
      account: <account>
      warehouse: <warehouse>
      database: <database>
```

#### BigQuery (Google Cloud)
- **Type** : Cloud Data Warehouse
- **Capacit√©s** : Metadata, Profiling, Usage, Lineage, Cost analysis
- **Configuration** :
```yaml
source:
  type: BigQuery
  serviceName: bigquery-prod
  serviceConnection:
    config:
      type: BigQuery
      credentials:
        gcpConfig:
          type: service_account
          projectId: <project_id>
          privateKeyId: <private_key_id>
          privateKey: <private_key>
          clientEmail: <client_email>
```

#### Redshift (AWS)
- **Type** : Cloud Data Warehouse
- **Capacit√©s** : Metadata, Profiling, Usage, Lineage
- **Configuration** :
```yaml
source:
  type: Redshift
  serviceName: redshift-prod
  serviceConnection:
    config:
      type: Redshift
      username: <username>
      password: <password>
      hostPort: <cluster>.redshift.amazonaws.com:5439
      database: <database>
```

### 1.2 NoSQL Databases

#### MongoDB
- **Type** : Document database
- **Capacit√©s** : Metadata, Sample data
- **Configuration** :
```yaml
source:
  type: MongoDB
  serviceName: mongodb-prod
  serviceConnection:
    config:
      type: MongoDB
      username: <username>
      password: <password>
      hostPort: localhost:27017
```

#### Cassandra
- **Type** : Wide-column store
- **Capacit√©s** : Metadata
- **Configuration** :
```yaml
source:
  type: Cassandra
  serviceName: cassandra-prod
  serviceConnection:
    config:
      type: Cassandra
      username: <username>
      password: <password>
      hostPort: localhost:9042
```

#### DynamoDB (AWS)
- **Type** : Key-value database
- **Capacit√©s** : Metadata, Table schemas
- **Configuration** :
```yaml
source:
  type: DynamoDB
  serviceName: dynamodb-prod
  serviceConnection:
    config:
      type: DynamoDB
      awsConfig:
        awsAccessKeyId: <access_key>
        awsSecretAccessKey: <secret_key>
        awsRegion: us-east-1
```

### 1.3 Storage Systems

#### S3 (AWS)
- **Type** : Object storage
- **Capacit√©s** : Metadata, File structure, Schema extraction
- **Configuration** :
```yaml
source:
  type: S3
  serviceName: s3-prod
  serviceConnection:
    config:
      type: S3
      awsConfig:
        awsAccessKeyId: <access_key>
        awsSecretAccessKey: <secret_key>
        awsRegion: us-east-1
```

#### HDFS
- **Type** : Distributed file system
- **Capacit√©s** : Metadata, File structure
- **Configuration** :
```yaml
source:
  type: HDFS
  serviceName: hdfs-prod
  serviceConnection:
    config:
      type: HDFS
      connectionOptions:
        hdfsURL: hdfs://namenode:9000
```

#### Azure Data Lake Storage (ADLS)
- **Type** : Cloud storage
- **Capacit√©s** : Metadata, File structure
- **Configuration** :
```yaml
source:
  type: ADLS
  serviceName: adls-prod
  serviceConnection:
    config:
      type: ADLS
      clientId: <client_id>
      clientSecret: <client_secret>
      tenantId: <tenant_id>
      accountName: <storage_account>
```

### 1.4 Data Platforms

#### Databricks
- **Type** : Unified analytics platform
- **Capacit√©s** : Metadata, Lineage, Notebooks, Jobs
- **Configuration** :
```yaml
source:
  type: Databricks
  serviceName: databricks-prod
  serviceConnection:
    config:
      type: Databricks
      token: <access_token>
      hostPort: <workspace>.cloud.databricks.com
      httpPath: /sql/1.0/warehouses/<warehouse_id>
```

#### Apache Hive
- **Type** : Data warehouse
- **Capacit√©s** : Metadata, Profiling, Lineage
- **Configuration** :
```yaml
source:
  type: Hive
  serviceName: hive-prod
  serviceConnection:
    config:
      type: Hive
      username: <username>
      password: <password>
      hostPort: localhost:10000
      databaseSchema: <database>
```

#### Apache Spark
- **Type** : Processing engine
- **Capacit√©s** : Metadata, Job tracking
- **Configuration** :
```yaml
source:
  type: Spark
  serviceName: spark-prod
  serviceConnection:
    config:
      type: Spark
      hostPort: <master_url>
      connectionArguments:
        spark.app.name: OpenMetadata
```

---

## üìà 2. Data Profiler Agent

### R√¥le
Analyse statistique des donn√©es pour √©valuer la qualit√©.

### Capacit√©s
- **M√©triques de table** : Nombre de lignes, colonnes
- **M√©triques de colonnes** : 
  - Null count / proportion
  - Distinct count / unique count
  - Min / Max / Mean / Median
  - Standard deviation
  - Quartiles (Q1, Q2, Q3)
- **M√©triques textuelles** :
  - Longueur min/max/moyenne
  - Patterns communs
- **M√©triques num√©riques** :
  - Distribution
  - Histogrammes

### Activation
```yaml
source:
  sourceConfig:
    config:
      type: DatabaseMetadata
      enableProfiler: true
      profileSample: 100  # % de donn√©es √† √©chantillonner
      profileQuery: ""    # Query custom optionnelle
```

### Support
- ‚úÖ Dremio (impl√©ment√©)
- ‚úÖ PostgreSQL
- ‚úÖ MySQL
- ‚úÖ Oracle
- ‚úÖ SQL Server
- ‚úÖ Snowflake
- ‚úÖ BigQuery
- ‚úÖ Redshift
- ‚ö†Ô∏è NoSQL (limit√©)

### Documentation
Voir `PROFILING_GUIDE.md` pour Dremio

---

## üè∑Ô∏è 3. Auto Classification Agent

### R√¥le
D√©tection automatique de donn√©es sensibles (PII, GDPR, etc.)

### Capacit√©s
- **D√©tection bas√©e sur patterns** : Noms de colonnes
- **D√©tection bas√©e sur donn√©es** : Regex sur valeurs
- **Classifications support√©es** :
  - PII (Personally Identifiable Information)
  - Sensitive Data
  - Financial Data
  - Healthcare (PHI)
  - Custom classifications

### Tags Standard
- **PII** : Email, Phone, Name, Address, SSN, ID
- **Sensitive** : Password, Token, API Key, Secret
- **Financial** : Credit Card, Bank Account, IBAN, SWIFT
- **Healthcare** : Medical Record Number, Patient ID

### Activation
```yaml
source:
  sourceConfig:
    config:
      type: DatabaseMetadata
      enableAutoClassification: true
      classificationFilterPattern:
        includes:
          - PII
          - Sensitive
          - Financial
```

### Support
- ‚úÖ Dremio (impl√©ment√©)
- ‚úÖ PostgreSQL
- ‚úÖ MySQL
- ‚úÖ Oracle
- ‚úÖ SQL Server
- ‚úÖ Snowflake
- ‚úÖ BigQuery
- ‚úÖ Redshift

### Documentation
Voir `CLASSIFICATION_GUIDE.md` pour Dremio

---

## üìä 4. Usage Agent (Query Analytics)

### R√¥le
Collecte les requ√™tes ex√©cut√©es et analyse l'utilisation des tables.

### Capacit√©s
- **Requ√™tes captur√©es** : 
  - Query text
  - User / Service account
  - Timestamp
  - Execution time
  - Rows scanned
- **M√©triques d'utilisation** :
  - Most queried tables
  - Query patterns
  - Peak usage times
  - User activity

### Activation
```yaml
source:
  type: Usage
  serviceName: <database>-usage
  sourceConfig:
    config:
      type: DatabaseUsage
      queryLogDuration: 7  # Jours de logs √† analyser
      stageFileLocation: /tmp/usage
```

### Support
- ‚úÖ Snowflake (excellent - query history API)
- ‚úÖ BigQuery (excellent - audit logs)
- ‚úÖ Redshift (excellent - system tables)
- ‚úÖ PostgreSQL (bon - pg_stat_statements)
- ‚ö†Ô∏è MySQL (limit√©)
- ‚ö†Ô∏è Dremio (API disponible mais pas encore impl√©ment√©)

### Exemple Snowflake
```yaml
source:
  type: Usage
  serviceName: snowflake-usage
  sourceConfig:
    config:
      type: DatabaseUsage
      queryLogDuration: 7
```

---

## üîó 5. Lineage Agent

### R√¥le
Tra√ßage des transformations de donn√©es (d'o√π viennent les donn√©es, o√π vont-elles).

### Capacit√©s
- **Table-to-table lineage** : Relations entre tables
- **Column-to-column lineage** : Transformations de colonnes
- **Query-based lineage** : Parsing des requ√™tes SQL
- **ETL lineage** : Tracking des pipelines

### Activation
```yaml
source:
  type: Lineage
  serviceName: <database>-lineage
  sourceConfig:
    config:
      type: DatabaseLineage
      queryLogDuration: 7
```

### Support
- ‚úÖ Snowflake (excellent)
- ‚úÖ BigQuery (excellent)
- ‚úÖ Databricks (excellent)
- ‚úÖ Redshift (bon)
- ‚ö†Ô∏è PostgreSQL (basique)
- ‚ö†Ô∏è MySQL (basique)
- ‚ùå Dremio (pas encore impl√©ment√©)

### Int√©gration DBT
Le lineage DBT est g√©r√© par l'agent DBT (voir `dags/dbt_dag.py`)

---

## üß™ 6. Data Quality Agent

### R√¥le
D√©finition et ex√©cution de tests de qualit√© de donn√©es.

### Types de Tests
- **Table Tests** :
  - Row count validation
  - Freshness check
  - Custom SQL assertions
- **Column Tests** :
  - Not null
  - Unique values
  - Values in range
  - Regex pattern matching
  - Custom SQL

### D√©finition des Tests
Via l'interface OpenMetadata :
1. S√©lectionner une table
2. Onglet "Data Quality"
3. "Add Test" ‚Üí Choisir le type
4. Configurer les param√®tres

### Ex√©cution
```yaml
source:
  type: TestSuite
  serviceName: <database>-tests
  sourceConfig:
    config:
      type: TestSuite
```

### Support
- ‚úÖ Tous les connecteurs JDBC
- ‚úÖ Dremio (via SQL)
- ‚úÖ Snowflake
- ‚úÖ BigQuery
- ‚úÖ Redshift

---

## üìã 7. Data Insights Agent

### R√¥le
G√©n√©ration de m√©triques et rapports sur l'ensemble du catalogue.

### M√©triques Collect√©es
- **Coverage** :
  - % de tables avec description
  - % de colonnes avec description
  - % de tables avec owner
- **Ownership** :
  - R√©partition par √©quipe
  - Tables sans owner
- **Data Quality** :
  - Tests pass√©s/√©chou√©s
  - Tendances qualit√©
- **Classification** :
  - % de colonnes classifi√©es
  - Distribution des tags PII

### Activation
Automatique - Pas de configuration n√©cessaire

### Acc√®s
UI : Insights ‚Üí Data Insights

---

## üîî 8. Webhook / Event Notification

### R√¥le
Notifications en temps r√©el sur les changements de m√©tadonn√©es.

### √âv√©nements Support√©s
- Table cr√©√©e/modifi√©e/supprim√©e
- Schema chang√©
- Owner assign√©
- Tag appliqu√©
- Test de qualit√© √©chou√©

### Configuration
```yaml
source:
  type: EventHub
  serviceName: notifications
  sourceConfig:
    config:
      type: EventHub
      endpoints:
        - http://your-webhook-url.com/metadata-events
```

### Destinations
- Webhooks HTTP
- Slack
- Microsoft Teams
- Email
- Custom handlers

---

## üìÅ 9. Messaging Systems Agents

### Apache Kafka
- **Capacit√©s** : Topics, Schemas, Consumer groups
- **Configuration** :
```yaml
source:
  type: Kafka
  serviceName: kafka-prod
  serviceConnection:
    config:
      type: Kafka
      bootstrapServers: localhost:9092
      schemaRegistryURL: http://localhost:8081
```

### Apache Pulsar
- **Capacit√©s** : Topics, Namespaces
- **Configuration** :
```yaml
source:
  type: Pulsar
  serviceName: pulsar-prod
  serviceConnection:
    config:
      type: Pulsar
      brokerUrl: pulsar://localhost:6650
      adminUrl: http://localhost:8080
```

### RabbitMQ
- **Capacit√©s** : Queues, Exchanges
- **Configuration** :
```yaml
source:
  type: RabbitMQ
  serviceName: rabbitmq-prod
  serviceConnection:
    config:
      type: RabbitMQ
      username: <username>
      password: <password>
      hostPort: localhost:5672
```

---

## üîß 10. Orchestration Systems

### Apache Airflow
- **Capacit√©s** : DAGs, Tasks, Lineage
- **Configuration** :
```yaml
source:
  type: Airflow
  serviceName: airflow-prod
  serviceConnection:
    config:
      type: Airflow
      hostPort: http://localhost:8080
      connection:
        type: Backend
```

### Dagster
- **Capacit√©s** : Jobs, Assets, Lineage
- **Configuration** :
```yaml
source:
  type: Dagster
  serviceName: dagster-prod
  serviceConnection:
    config:
      type: Dagster
      host: localhost
      port: 3000
```

### Prefect
- **Capacit√©s** : Flows, Tasks
- **Configuration** :
```yaml
source:
  type: Prefect
  serviceName: prefect-prod
  serviceConnection:
    config:
      type: Prefect
      hostPort: http://localhost:4200
```

---

## üìä 11. BI Tools

### Tableau
- **Capacit√©s** : Dashboards, Workbooks, Charts, Lineage
- **Configuration** :
```yaml
source:
  type: Tableau
  serviceName: tableau-prod
  serviceConnection:
    config:
      type: Tableau
      hostPort: https://tableau.company.com
      username: <username>
      password: <password>
      siteName: <site>
```

### Power BI
- **Capacit√©s** : Reports, Dashboards, Datasets, Lineage
- **Configuration** :
```yaml
source:
  type: PowerBI
  serviceName: powerbi-prod
  serviceConnection:
    config:
      type: PowerBI
      clientId: <client_id>
      clientSecret: <client_secret>
      tenantId: <tenant_id>
```

### Looker
- **Capacit√©s** : Looks, Dashboards, Explores, Lineage
- **Configuration** :
```yaml
source:
  type: Looker
  serviceName: looker-prod
  serviceConnection:
    config:
      type: Looker
      clientId: <client_id>
      clientSecret: <client_secret>
      hostPort: https://looker.company.com
```

### Superset
- **Capacit√©s** : Dashboards, Charts, Datasets
- **Configuration** :
```yaml
source:
  type: Superset
  serviceName: superset-prod
  serviceConnection:
    config:
      type: Superset
      hostPort: http://localhost:8088
      connection:
        username: <username>
        password: <password>
```

---

## üéØ Agents Recommand√©s pour Votre Architecture

### D√©j√† Actifs ‚úÖ
1. **Dremio Metadata** - Metadata + Profiling + Classification
2. **DBT** - Transformations + Lineage + Tests

### √Ä Ajouter üéØ

#### Haute Priorit√©
1. **PostgreSQL Metadata** - Pour PostgreSQL_BusinessDB
   - Pourquoi : Compl√®te les donn√©es Dremio avec source originale
   - B√©n√©fice : Double validation, d√©tection des changements
   
2. **MinIO/S3 Metadata** - Pour le bucket MinIO
   - Pourquoi : Tra√ßage des fichiers sources (CSV, Parquet)
   - B√©n√©fice : Lineage complet (fichier ‚Üí table)

3. **Usage Agent (Dremio)** - Query analytics
   - Pourquoi : Identifier les tables les plus utilis√©es
   - B√©n√©fice : Optimisation des performances

#### Priorit√© Moyenne
4. **Airflow Metadata** - DAGs actuels
   - Pourquoi : Documentation des workflows
   - B√©n√©fice : Lineage orchestration

5. **Superset Metadata** - Dashboards (si utilis√©)
   - Pourquoi : Tra√ßabilit√© des visualisations
   - B√©n√©fice : Lineage end-to-end

#### Priorit√© Basse
6. **Data Quality Agent** - Tests custom
   - Pourquoi : Validation automatique des donn√©es
   - B√©n√©fice : Alertes qualit√©

---

## üìã R√©capitulatif par Cat√©gorie

| Cat√©gorie | Agents Disponibles | Impl√©ment√©s | √Ä Ajouter |
|-----------|-------------------|-------------|-----------|
| **Databases** | 15+ | Dremio ‚úÖ | PostgreSQL üéØ |
| **Storage** | 3 | - | MinIO/S3 üéØ |
| **Profiling** | 10+ | Dremio ‚úÖ | - |
| **Classification** | 10+ | Dremio ‚úÖ | - |
| **Usage** | 5 | - | Dremio üéØ |
| **Lineage** | 5 | DBT ‚úÖ | - |
| **Data Quality** | Tous | - | Custom üí° |
| **Orchestration** | 3 | - | Airflow üí° |
| **BI Tools** | 4 | - | Superset üí° |
| **Messaging** | 3 | - | - |

**L√©gende** :
- ‚úÖ D√©j√† impl√©ment√© et actif
- üéØ Recommand√© √† ajouter en priorit√©
- üí° √Ä consid√©rer selon besoins

---

## üöÄ Prochaines √âtapes

### Phase 1 : Activer les fonctionnalit√©s Dremio (Imm√©diat)
1. ‚úÖ Metadata - D√©j√† actif
2. ‚è≥ Profiling - √Ä activer dans l'UI
3. ‚è≥ Classification - √Ä activer dans l'UI

### Phase 2 : Ajouter PostgreSQL (1-2 jours)
1. Cr√©er connecteur PostgreSQL dans OpenMetadata
2. Configurer l'ingestion metadata
3. Activer le profiling
4. Comparer avec Dremio pour validation

### Phase 3 : Ajouter MinIO/S3 (1-2 jours)
1. Configurer connecteur S3 (compatible MinIO)
2. Scanner le bucket opendata
3. √âtablir lineage fichiers ‚Üí tables

### Phase 4 : Usage Analytics (1 semaine)
1. Impl√©menter collecte query logs Dremio
2. Cr√©er agent Usage custom
3. Dashboard d'utilisation

### Phase 5 : Orchestration (optionnel)
1. Connecter Airflow actuel
2. Documenter les DAGs
3. Lineage orchestration

---

## üìö Ressources

- **Documentation OpenMetadata** : https://docs.open-metadata.org/
- **Connectors List** : https://docs.open-metadata.org/connectors
- **API Reference** : https://docs.open-metadata.org/sdk/python

---

‚úÖ **Document cr√©√© le** : 2025-10-20
üìù **Version** : 1.0
üéØ **Objectif** : Cartographie compl√®te des agents disponibles pour enrichir le catalogue OpenMetadata
